# Components

**Components are building blocks for agentic workflows.** LlamaIndex has many components, but we'll focus on the key ones for building agentic workflows, including the `QueryEngine`, `Agent` and `Tool` components.

Many of the components rely on integrations with other frameworks. So, before using them, we first need to learn how to install them as dependencies.

## Integrations

### Installation

Most frameworks add their installation guide to their main documentation but LlamaIndex keep a well structured overview in their [GitHub repository](https://github.com/run-llama/llama_index/tree/main/llama-index-integrations).
This might be a bit overwhelming at first, but the **installation commands generally follow an easy to remember format**:

```bash
pip install llama-index-{component-type}-{framework-name}
```

Let's try install the depencies for an LLM and embedding component using Hugging Face inference API as framework.

```bash
pip install llama-index-llms-huggingface-api llama-index-embeddings-huggingface-api
```

### Usage

Once installed, we can use the component in our workflow. The usage patterns have been outlined in [the documentation](https://docs.llamaindex.ai/en/stable/module_guides/) but framework specific versions are also shown in the [GitHub repository](https://github.com/run-llama/llama_index/tree/main/llama-index-integrations).
Underneath, we can see an example of the usage of the Hugging Face inference API for an LLM component.

```python
from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI

llm = HuggingFaceInferenceAPI(
    model_name="openai-community/gpt2",
    temperature=0.7,
    max_tokens=100,
    token="<your-token>",  # Optional
)

llm.complete("Hello, how are you?")
# I am good, how can I help you today?
```

Now, let's dive a bit deeper into the components and see how they work in context.

## Components for a QueryEngine

Instead of going over each of the components one by one, we will take a look at **how to use components in context within three common scenarios**.

1. creating an index from a set of documents
2. querying an index with prompts and LLMs

TODO: Add image of Alfred

### 1. Creating an index from a set of documents

We will work with the [Loading](https://docs.llamaindex.ai/en/stable/module_guides/loading/), [Embedding](https://docs.llamaindex.ai/en/stable/module_guides/embedding/), [Indexing](https://docs.llamaindex.ai/en/stable/module_guides/indexing/) and [Storing](https://docs.llamaindex.ai/en/stable/module_guides/storing/) components to create a document index.

As mentioned before, LlamaIndex can work on top of your own data, however, **before accessing data, we need to load it into the library.**
There are three main ways to do to load data into LlamaIndex:

1. [`SimpleDirectoryReader`](https://docs.llamaindex.ai/en/stable/module_guides/loading/simpledirectoryreader/): A built-in loader for various file types from a local directory.
2. [`LlamaParse`](https://docs.llamaindex.ai/en/stable/module_guides/loading/connector/llama_parse/): LlamaParse, LlamaIndex's official tool for PDF parsing, available as a managed API.
3. [`LlamaHub`](https://docs.llamaindex.ai/en/stable/module_guides/loading/connector/): A registry of hundreds of data loading libraries to ingest data from any source.

Let's start with the most basic and intuitive way to load data, using the `SimpleDirectoryReader`. This loader allows us to **load various file types from a local directory and loads them as `Document` objects.**

```python
from llama_index.core import SimpleDirectoryReader

reader = SimpleDirectoryReader(input_dir="path/to/directory")
documents = reader.load_data()
```

Now, we have access to our `Document` objects and can **load and transform them into `Node` objects, which represents "chunks" of a source `Document`.**  They can often be used interchangeably with Documents, and similar to `Documents`, they contain metadata and relationship information with other nodes.
**The quickest way to create `Node` objects is using the [`IngestionPipeline`](https://docs.llamaindex.ai/en/stable/module_guides/loading/ingestion_pipeline/)** class with [`NodeParser`](https://docs.llamaindex.ai/en/stable/module_guides/loading/node_parsers/modules/) and [`Embeddings`](https://docs.llamaindex.ai/en/stable/module_guides/models/embeddings/) transformations.
We will use a basic `SentenceSplitter` to split the `Document` into chunks of 25 sentences with no overlap and use `HuggingFaceInferenceAPIEmbedding` to add vectors to the `Node` objects.

```python
from llama_index.core import Document
from llama_index.embeddings.huggingface_api import HuggingFaceInferenceAPIEmbedding
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.ingestion import IngestionPipeline, IngestionCache

# create the pipeline with transformations
pipeline = IngestionPipeline(
    transformations=[
        SentenceSplitter(chunk_size=25, chunk_overlap=0),
        HuggingFaceInferenceAPIEmbedding("BAAI/bge-small-en-v1.5"),
    ]
)

# run the pipeline
nodes = pipeline.run(documents=[Document.example()])
```

> Loading and embedding can be computationally expensive, so LlamaIndex provides a way to cache the results of the ingestion pipeline, so we don't have to compute them again.

Now that we have our `Node` objects, we need to store them somewhere. LlamaIndex lets us store nodes in different ways:

- A simple `DocumentStore` for basic storage
- A `VectorStore` for similarity search
- Or one of [the other storage options](https://docs.llamaindex.ai/en/stable/module_guides/storing/)

Don't worry too much about which storage to pick - LlamaIndex handles this for you with something called a `StorageContext`. You can save and load this context whenever you need it.

For getting started, the `VectorStoreIndex` is your best bet. It allows you to store everything in memory which makes it fast and easy to experiment with.

Let's create a `VectorStoreIndex` from our `Node` objects and persist it to disk by saving the `StorageContext`.

```python
from llama_index.core import VectorStoreIndex

index = VectorStoreIndex.from_documents(nodes)
index.storage_context.persist("path/to/vector/store")
```

Now, we can load the index using through the `StorageContext` and use it in our workflow. Make sure to also pass in the `embed_model` to the `VectorStoreIndex` constructor to search for embeddings in the same vector space.

```python
from llama_index.core import StorageContext, load_index_from_storage
from huggingface_api import HuggingFaceInferenceAPIEmbedding

embed_model = HuggingFaceInferenceAPIEmbedding("BAAI/bge-small-en-v1.5")
storage_context = StorageContext.from_defaults(persist_dir="path/to/vector/store")
index = load_index_from_storage(storage_context, embed_model=embed_model)
```

TODO: Add image of Alfred

Great! Now that we can save and load our index easily, let's explore how to query it in different ways.

### 2. Querying an index with prompts and LLMs

Before querying our index, we need to convert it to a query interface. The most common options are:

- `as_retriever`: For basic document retrieval, returning a list of `NodeWithScore` objects with similarity scores
- `as_query_engine`: For single question-answer interactions, returning a written response
- `as_chat_engine`: For conversational interactions that maintain memory across multiple messages, returning a chat history

We'll focus on the query engine since it is more common for agent-like interactions.
We also pass in an LLM to the query engine to use for the response.

```python
from llama_index.llms.huggingface_api import HuggingFaceInferenceAPILM

llm = HuggingFaceInferenceAPILM(model_name="meta-llama/Meta-Llama-3-8B-Instruct")
query_engine = index.as_query_engine(llm=llm)
query_engine.query("What is the meaning of life?")
# the meaning of life is 42
```

Under the hood, the query engine doesn't only use the LLM to answer the question, but also uses a `ResponseSynthesizer` as strategy to process the response.
Once again, this is fully customisable but there are three main strategies that work well out of the box:

- `refine`: create and refine an answer by sequentially going through each retrieved text chunk. This makes a separate LLM call per Node/retrieved chunk.
- `compact` (default): similar to refine but concatenate the chunks beforehand, resulting in less LLM calls.
- `tree_summarize`: create a detailed answer by going through each retrieved text chunk and creating a tree structure of the answer.

TODO: Add image of Alfred

> Take fine-grained control of your query workflows with the [low-level composition API](https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/usage_pattern/#low-level-composition-api). This API lets you customize and fine-tune every step of the query process to match your exact needs.


### 3. Using Agents and Tools


