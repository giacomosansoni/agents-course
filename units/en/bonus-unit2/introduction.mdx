# AI Agent Observability & Evaluation

![Bonus Unit 2 Thumbnail](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/thumbnail.jpg)

Welcome to **Bonus Unit 2**! In this chapter, you'll explore advanced strategies for observing, evaluating, and ultimately improving the performance of your agents.

---

## When Should I Do This Bonus Unit?

This bonus unit is perfect if you:
- **Develop and Deploy AI Agents:** You want to ensure that your agents are performing reliably in production.
- **Need Detailed Insights:** You're looking to diagnose issues, optimize performance, or understand the inner workings of your agent.
- **Aim to Reduce Operational Overhead:** By monitoring agent costs, latency, and execution details, you can efficiently manage resources.
- **Seek Continuous Improvement:** You’re interested in integrating both real-time user feedback and automated evaluation into your AI applications.

In short, for everyone who wants to bring their agents in front of users!

---

## What You’ll Learn

In this unit, you'll discover how to:
- **Instrument Your Agent:** Learn how to integrate observability tools like OpenTelemetry and Langfuse with the *smolagents* framework.
- **Monitor Critical Metrics:** Track performance indicators such as token usage (costs), latency, and error traces.
- **Evaluate in Real-Time:** Understand techniques for live evaluation, including gathering user feedback and leveraging an LLM-as-a-judge.
- **Perform Offline Analysis:** Utilize benchmark datasets (e.g., GSM8K) to systematically test and compare agent performance.
- **Leverage Tracing Insights:** Dive into the breakdown of agent operations—identifying tool calls, LLM interactions, and other internal steps—to pinpoint optimization opportunities.

---

## By the End of This Bonus Unit, You’ll Be Able To

- **Integrate Observability Tools:** Seamlessly set up and configure OpenTelemetry-based instrumentation for your agents.
- **Monitor and Analyze Agent Runs:** Capture detailed traces that reveal performance metrics such as costs, latency, and token usage.
- **Implement Evaluation Strategies:** Utilize both online (user feedback, automated evaluations) and offline (benchmark dataset runs) evaluation methods.
- **Diagnose and Optimize:** Use collected data and traces to debug issues, refine performance, and ensure your agent operates at its best.

---

## Ready to Get Started?

Dive into a hands-on notebook that walks you through real-world examples of agent monitoring and evaluation. Click the link below to open the [Monitoring and Evaluation Notebook](../notebooks/bonus-unit2/monitoring-agents.ipynb) and start exploring how observability can transform your AI agent development.

Happy coding and insightful monitoring!
