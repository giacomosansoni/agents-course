# What is an Agent?

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/unit1-whiteboard-start.jpg" alt="Unit 1 planning"/>

Since you are interested in learning more about **Agents**, here is the moment to discuss the fundamental question: **what is an Agent?**

To explain this concept, let's start with an analogy.

## The Big Picture: Alfred The Agent

Meet Alfred. Alfred is an **Agent**.

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/this-is-alfred.jpg" alt="This is Alfred"/>

Imagine Alfred receives a command, such as: "Alfred, I would like a coffee please."

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/coffee-please.jpg" alt="I would like a coffee"/>

Because Alfred **understands natural language**, he quickly grasps our request.

Before fulfilling the order, Alfred engages in **reasoning and planning** to define the step of actions he needs to make, and which tools to use.

Alfred defined his plan: go to the kitchen -> select the tool to make the coffee -> make the coffee -> bring the coffee back to us. 

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/reason-and-plan.jpg" alt="Reason and plan"/>

Once he has a plan in mind **he must act**. 

To execute his plan, **he can use tools in the list of tools he knows about**. In this case, to make a coffee, he uses a coffee machine. He activates the coffee machine to brew the coffee.

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/make-coffee.jpg" alt="Make coffee"/>

Finally, Alfred brings the freshly made coffee to us.

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/bring-coffee.jpg" alt="Bring coffee"/>

And this is what an Agent is: an AI model capable of reasoning, planning, and interacting with its environment. We call it Agent because it has _agency_, aka it has the ability to interact with the environment.


<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/process.jpg" alt="Agent process"/>

<!-- Maybe step 2 could be: select tools, then Step 3: Act -->

## Let's go more formal

Now that we understood the big picture, let's formally define an agent as follows :

> An Agent is a system that leverages an AI model to interact with its environment in order to achieve a user-defined objective. It combines reasoning, planning, and the execution of actions (often via external tools) to fulfill tasks.

The AI model can be seen as the **brain of the agent** and the framework as the **remaining body parts**:

- The AI model does the reasoning and will then send **Actions** to execute.
- The scope of what is possible **depends on what the model has been equipped with**. It's like a human by not having "wings" can't execute the "fly" **Action**, while it is possible to execute **Actions** "walk", "run" ,"jump", "grab", and so on.

## What type of AI Models do we use for Agents?

The most commonly AI model found at the core of an Agent is an LLM (Large Language Model). This is a kind of AI model that takes **Text** as an input and Also outputs **Text**.

Well known representatives are **GPT4** from **OpenAI**, **LLama** from **meta**, **Gemini** from **Google**, etc... Those models have been trained on a very vast amount of text and are able to generalize well. We will learn more about LLMs in [the next section](what-are-llms).

Note: it's possible to use slightly different models as the Agent's core. One example is a Vision Language Model (VLM), which is like an LLM but also understands images as input. We'll focus on LLMs for now and will discuss other options later.


## How does an AI take action on its environment ?

LLMs are amazing models, but they can only generate text. However, if you have used chat applications like HuggingChat, ChatGPT or Le Chat, you will have noticed that they are able to generate images too! How is that possible?

The answer is that the developers of those applications created additional code (called **Tools**), that the LLM can use to create an image.

<figure>
<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/eiffel_brocolis.jpg" alt="Eiffel Brocolis"/>
<figcaption>The model used an Image Generation Tool to generate this image.
</figcaption>
</figure>

We will learn more about tools in the [Tools](tools) section.


## What type of tasks can an Agent do?

An Agent can perform any tasks, as long as we provide it with **Tools** to perform **Actions**.

For example, if I write an Agent to act as my personal assistant (like Siri) on my computer, and I ask it to "send an email to my Manager asking to delay today's meeting", I can give it some code to send emails. This will be a new Tool the Agent can use whenever it needs to send an email. We can write it in Python:

```python
def send_message_to(recipient, message):
    """Useful to send an e-mail message to a recipient"""
```

The LLM, as we'll see, will generate code to run the tool when it needs to, and thus fulfill the desired task.

```python
send_message_to("Manager", "Can we postopone today's meeting?")
```

The **design of the Tools is very important and has a great impact on the quality of your Agent**. Some tasks will require very specific tools to be crafted, while others may be solved with general purpose tools like "web_search".

Note that Actions are not the same as Tools. An Action, for instance, can involve the use of multiple Tools to complete.

Having an AI interact with its environment opens a lot of real-life scenarios for companies and individuals.

### Example 1: Personal Virtual Assistants

Virtual assistants like Siri, Alexa, or Google Assistant, work as agents when they interact on behalf of users using their digital environments.

They take user queries, analyze context, retrieve information from databases, and provide responses or initiate actions (like setting reminders, sending messages, or controlling smart devices).

### Example 2: Customer Service Chatbots

Many companies deploy chatbots as agents that interact with customers in natural language. These agents can answer questions, guide users through troubleshooting steps, open issues in internal databases, or even complete transactions.

Their predefined objectives might include improving user satisfaction, reducing wait times, or increasing sales conversion rates. By interacting directly with customers, learning from the dialogues, and adapting their responses over time, they demonstrate the core principles of an agent in action.


### Example 3: AI NPC (Non-Playable Characte) in a video game

AI agents powered by LLMs can make NPCs more dynamic and unpredictable. Instead of following rigid behavior trees, they can respond contextually, adapt to player interactions, and generate more nuanced dialogue. This flexibility helps create more lifelike, engaging characters that evolve alongside the playerâ€™s actions.

To summarize, an Agent is a system that uses an AI Model (mostly an LLM) as its core reasoning engine, to:

- **Understand natural language:**  Interpret and respond to human instructions in a meaningful way.
- **Reason and plan:** Analyze information, make decisions, and devise strategies to solve problems.
- **Interact with its environment:**  Gather information, take actions, and observe the results of those actions.

Now that we understood what Agents are, we need to dive deeper into the "Agent's brain": the [LLMs](what-are-llms).
