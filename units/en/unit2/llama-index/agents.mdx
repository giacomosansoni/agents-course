# Using Agents in LlamaIndex

Now we've gotten a basic understanding of what tools are in LlamaIndex, let's dive into how we can use agents in LlamaIndex.
First, let's recap the definition of an Agent introduced in unit 1:

> An Agent is a system that leverages an AI model to interact with its environment in order to achieve a user-defined objective. It combines reasoning, planning, and the execution of actions (often via external tools) to fulfill tasks.

LlamaIndex supports three main types of reasoning agents:

1. Function Calling Agents - These work with AI models that can call specific functions
2. ReAct Agents - These can work with any AI that does chat or text endpoint and deal with complex reasoning tasks
3. Advanced Agents - These use more complex methods like LLMCompiler or Chain-of-Abstraction

<Tip>Find more information on advanced agents on [LlamaIndex GitHub](https://github.com/run-llama/llama_index/tree/main/llama-index-packs)</Tip>

An agent is initialized from a set of Tools. Here's an example of instantiating a ReAct agent from a set of Tools.

```python
from llama_index.core.tools import FunctionTool
from llama_index.llms.huggingface_api import HuggingFaceInferenceAPILM
from llama_index.core.agent import ReActAgent

# define sample Tool
def multiply(a: int, b: int) -> int:
    """Multiple two integers and returns the result integer"""
    return a * b

multiply_tool = FunctionTool.from_defaults(fn=multiply)

# initialize llm
llm = HuggingFaceInferenceAPILM(model_name="meta-llama/Meta-Llama-3-8B-Instruct")

# initialize ReAct agent
agent = ReActAgent.from_tools([multiply_tool], llm=llm, verbose=True)
```

Similarly, we can use the `AgentRunner` to automatically pick the best agent reasoning flow depending on the LLM.

```python
from llama_index.core.agent import AgentRunner

agent_runner = AgentRunner.from_llm(llm, verbose=True)
```

Agent supports both chat and query endpoints with `query()` and `chat()`, where chat interactions keep a history of messages.

```python
response = agent.query("What is 2 times 2?")
```

Now we've gotten the basics, let's take a look at how we can use tools in our agents.

## Creating RAG Agents with QueryEngineTools

It is easy to **wrap `QueryEngine` as tools** for an agent.
When doing so, we need to **define a name and description within the `ToolMetadata`**. The LLM will use this information to correctly use the tool.
Let's see how to load in a `QueryEngineTool` using the `QueryEngine` we created in the [component section](02_components.mdx).

```python
from llama_index.core.tools import QueryEngineTool, ToolMetadata

query_engine = index.as_query_engine(similarity_top_k=3) # as shown in the previous section

query_engine_tool = QueryEngineTool(
    query_engine=query_engine,
    metadata=ToolMetadata(
        name="a specific name",
        description="a specific description",
    ),
    return_direct=False,
)
query_engine_agent = ReActAgent.from_tools([query_engine_tool], llm=llm, verbose=True)
```

## Creating Multi-agent systems

**Agents in LlamaIndex can directly be used as tools** for other agents by loading them as a `QueryEngineTool`.

```python
from llama_index.core.tools import QueryEngineTool

query_engine_agent = # as defined in the previous section

query_engine_agent_tool = QueryEngineTool(
    query_engine=query_engine_agent,
    metadata=ToolMetadata(
        name="a specific name",
        description="a specific description",
    ),
)

multi_agent = ReActAgent.from_tools([query_engine_agent_tool], llm=llm, verbose=True)
```

<Tip>Haven't learned enough yet? There is a lot more to discover about agents and tools in LlamaIndex within the [Agent Component Guide](https://docs.llamaindex.ai/en/stable/module_guides/deploying/agents/modules/) or the [Agent Learning Guide](https://docs.llamaindex.ai/en/stable/understanding/agent/).</Tip>

Now that we understand the basics of agents and tools in LlamaIndex, let's see how we can use LlamaIndex to create configurable and manageable workflows!