# Fine-tuning
## How do we train our model for function-calling ?

> Answer : The necessicity for this is **data**

A model training can be seen in 3 steps :
- 1) The model is pretrained on a large quantity of data. The output of that step is a **pre-trained model**.
- 2) The model can be **fine-tuned** on instruction following either by the model creator or by an individual ( or both ).
- 3) The model can be **aligned** to the creator's preference.

Usually a full fledged product like Gemini or mistral went throught all 3 steps while the models you can find on Hugging Face have passed by one or more steps of this training.

In this tutorial, we will build a function-calling model based on **"google/gemma-2-2b-it"**. 

The base model is "google/gemma-2-2b". The google team then fine-tuned the base model on instruction following : resulting in **"google/gemma-2-2b-it"**. In this case we will take **"google/gemma-2-2b-it"** as base and not the base model because the prior fine-tuning it has been through is important for our use-case.

Since we want to interract with our model throught conversations in messages, starting from the base model would requiere more training in order to learn instruction following, chat AND function-calling.
By taking the instruct-tuned model as a base, we minimize the amount of information that our model should learn.

### LoRA  (Low-Rank Adaptation of Large Language Models) :
LoRA (Low-Rank Adaptation of Large Language Models) is a popular and lightweight training technique that significantly reduces the number of trainable parameters. It works by inserting a smaller number of new weights as an adapter into the model and only these are trained. 
This makes training with LoRA much faster, memory-efficient, and produces smaller model weights (a few hundred MBs), which are easier to store and share. 

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/blog_multi-lora-serving_LoRA.gif" alt="LoRA inference" width="50%"/>

LoRA works by adding pairs of rank decomposition matrices to transformer layers, typically focusing on linear layers.
During training, we will "freeze" the base model and will only updates the weights of those newly added adapters. By doing so, the number of trainableparamters considerably drops as we now only need to update a fraction of the initial weights of the model.
During inference, the input is passed into the adapter and the base model or these adapter weights can be merged with the base model, resulting in no additional latency overhead. 

LoRA is particularly useful for adapting **large** language models to specific tasks or domains while keeping resource requirements manageable.
This helps reduce drastically the memory requiered to train a model.
