# Building and Integrating Tools for Your Agent

In this section, we'll be giving Alfred access to the web to help him find the latest news and information about the world. 
Additionally. we will give Alfred acess to weather details and models downloads statistics.

## Give Your Agent Access to the Web

Remember that we want Alfred to establish his presence as a true renaissance host, with a deep knowledge of the world.

To do so, we need to make sure that Alfred has access to the latest news and information about the world.

Let's start by creating a web search tool for Alfred!

<hfoptions id="agents-frameworks">
<hfoption id="smolagents">

```python
from smolagents import DuckDuckGoSearchTool

# Initialize the DuckDuckGo search tool
search_tool = DuckDuckGoSearchTool()

# Example usage
results = search_tool("Who's the current President of France?")
print(results)
```

Expected output:

```python
The current President of France in Emmanuel Macron.
```


</hfoption>
<hfoption id="llama-index">

```python
from llama_index.core.tools import FunctionTool
from duckduckgo_search import DDGS

def get_search_info(query: str, max_results: int = 10) -> str:
    ddgs = DDGS()
    results = ddgs.text(query, max_results=max_results)
    if len(results) == 0:
        raise Exception("No results found! Try a less restrictive/shorter query.")
    postprocessed_results = [f"[{result['title']}]({result['href']})\n{result['body']}" for result in results]
    return "## Search Results\n\n" + "\n\n".join(postprocessed_results)

# Initialize the DuckDuckGo search tool
search_tool = FunctionTool.from_defaults(get_search_info)
# Example usage
print(search_tool("Who's the current President of France?"))
```

Expected output:

```python
The current President of France in Emmanuel Macron.
```

</hfoption>
<hfoption id="langgraph">

```python
from langchain_community.tools import DuckDuckGoSearchRun

search_tool = DuckDuckGoSearchRun()
results = search_tool.invoke("Who's the current President of France?")
print(results)
```

Expected output:

```python
The current President of France in Emmanuel Macron.
```

</hfoption>
</hfoptions>

## Creating a Custom Tool for Weather Information to Schedule the Fireworks

Remember that we need to make sure the fireworks are not cancelled due to bad weather?

Let's create a custom tool that can be used to call an external weather API and get the weather information for a given location.

<Tip>
For the sake of simplicity, we're using a dummy weather API for this example. If you want to use a real weather API, you could implement a weather tool that uses the OpenWeatherMap API, like in [Unit 1](../unit1/tutorial.mdx).
</Tip>

<hfoptions id="agents-frameworks">
<hfoption id="smolagents">

```python
from smolagents import Tool
import random

class WeatherInfoTool(Tool):
    name = "weather_info"
    description = "Fetches dummy weather information for a given location."
    inputs = {
        "location": {
            "type": "string",
            "description": "The location to get weather information for."
        }
    }
    output_type = "string"

    def forward(self, location: str):
        # Dummy weather data
        weather_conditions = [
            {"condition": "Rainy", "temp_c": 15},
            {"condition": "Clear", "temp_c": 25},
            {"condition": "Windy", "temp_c": 20}
        ]
        # Randomly select a weather condition
        data = random.choice(weather_conditions)
        return f"Weather in {location}: {data['condition']}, {data['temp_c']}Â°C"

# Initialize the tool
weather_info_tool = WeatherInfoTool()
```

</hfoption>
<hfoption id="llama-index">

```python
import random
from llama_index.core.tools import FunctionTool

def get_weather_info(location: str) -> str:
    """Fetches dummy weather information for a given location."""
    # Dummy weather data
    weather_conditions = [
        {"condition": "Rainy", "temp_c": 15},
        {"condition": "Clear", "temp_c": 25},
        {"condition": "Windy", "temp_c": 20}
    ]
    # Randomly select a weather condition
    data = random.choice(weather_conditions)
    return f"Weather in {location}: {data['condition']}, {data['temp_c']}Â°C"

# Initialize the tool
weather_info_tool = FunctionTool.from_defaults(get_weather_info)

print(weather_info_tool('Paris'))
```

</hfoption>
<hfoption id="langgraph">

```python
from langchain.tools import Tool
import random

def get_weather_info(location: str) -> str:
    """Fetches dummy weather information for a given location."""
    # Dummy weather data
    weather_conditions = [
        {"condition": "Rainy", "temp_c": 15},
        {"condition": "Clear", "temp_c": 25},
        {"condition": "Windy", "temp_c": 20}
    ]
    # Randomly select a weather condition
    data = random.choice(weather_conditions)
    return f"Weather in {location}: {data['condition']}, {data['temp_c']}Â°C"

weather_info_tool = Tool(
    name="get_weather_info",
    func=get_weather_info,
    description="Fetches dummy weather information for a given location."
)

print(weather_info_tool.invoke('Paris'))
```

</hfoption>
</hfoptions>

## Creating a Hub Stats Tool for Influential AI Builders

Alfred can impress influential AI builders by discussing their most popular models. We'll create a tool to fetch model statistics from the Hugging Face Hub based on a username.

<hfoptions id="agents-frameworks">
<hfoption id="smolagents">

```python
from smolagents import Tool
from huggingface_hub import list_models

class HubStatsTool(Tool):
    name = "hub_stats"
    description = "Fetches the most downloaded model from a specific author on the Hugging Face Hub."
    inputs = {
        "author": {
            "type": "string",
            "description": "The username of the model author/organization to find models from."
        }
    }
    output_type = "string"

    def forward(self, author: str):
        try:
            # List models from the specified author, sorted by downloads
            models = list(list_models(author=author, sort="downloads", direction=-1, limit=1))
            
            if models:
                model = models[0]
                return f"The most downloaded model by {author} is {model.id} with {model.downloads:,} downloads."
            else:
                return f"No models found for author {author}."
        except Exception as e:
            return f"Error fetching models for {author}: {str(e)}"

# Initialize the tool
hub_stats_tool = HubStatsTool()

# Example usage
print(hub_stats_tool("facebook")) # Example: Get the most downloaded model by Facebook
```

Expected output:

```Python
The most downloaded model by facebook is facebook/esmfold_v1 with 12,544,550 downloads.
```

</hfoption>
<hfoption id="llama-index">

```python
import random
from llama_index.core.tools import FunctionTool
from huggingface_hub import list_models

def get_hub_stats(author: str) -> str:
    """Fetches the most downloaded model from a specific author on the Hugging Face Hub."""
    try:
        # List models from the specified author, sorted by downloads
        models = list(list_models(author=author, sort="downloads", direction=-1, limit=1))

        if models:
            model = models[0]
            return f"The most downloaded model by {author} is {model.id} with {model.downloads:,} downloads."
        else:
            return f"No models found for author {author}."
    except Exception as e:
        return f"Error fetching models for {author}: {str(e)}"

# Initialize the tool
hub_stats_tool = FunctionTool.from_defaults(get_hub_stats)

# Example usage
print(hub_stats_tool("facebook")) # Example: Get the most downloaded model by Facebook
```

Expected output:

```python
The most downloaded model by facebook is facebook/esmfold_v1 with 12,544,550 downloads.
```

</hfoption>
<hfoption id="langgraph">

```python
from langchain.tools import Tool
from huggingface_hub import list_models

def get_hub_stats(author: str) -> str:
    """Fetches the most downloaded model from a specific author on the Hugging Face Hub."""
    try:
        # List models from the specified author, sorted by downloads
        models = list(list_models(author=author, sort="downloads", direction=-1, limit=1))

        if models:
            model = models[0]
            return f"The most downloaded model by {author} is {model.id} with {model.downloads:,} downloads."
        else:
            return f"No models found for author {author}."
    except Exception as e:
        return f"Error fetching models for {author}: {str(e)}"

# Initialize the tool
hub_stats_tool = Tool(
    name="get_hub_stats",
    func=get_hub_stats,
    description="Fetches the most downloaded model from a specific author on the Hugging Face Hub."
)

# Example usage
print(hub_stats_tool("facebook")) # Example: Get the most downloaded model by Facebook
```

Expected output:

```python
The most downloaded model by facebook is facebook/esmfold_v1 with 13,109,861 downloads.
```

</hfoption>
</hfoptions>

With the Hub Stats Tool, Alfred can now impress influential AI builders by discussing their most popular models.

## Integrating Tools with Alfred

Now that we have all the tools, let's integrate them into Alfred's agent:

<hfoptions id="agents-frameworks">
<hfoption id="smolagents">

```python
from smolagents import CodeAgent, HfApiModel

# Initialize the Hugging Face model
model = HfApiModel()

# Create Alfred with all the tools
alfred = CodeAgent(
    tools=[search_tool, weather_info_tool, hub_stats_tool], 
    model=model
)

# Example query Alfred might receive during the gala
response = alfred.run("Who is Meta AI and what's their most popular model?")

print("ðŸŽ© Alfred's Response:")
print(response)
```

Expected output: 

```python
ðŸŽ© Alfred's Response:
Meta or previously facebook is an American multinational technology conglomerate.
The most downloaded model by facebook is facebook/esmfold_v1 with 12.195.721 downloads.
```

</hfoption>
<hfoption id="llama-index">

```python
from llama_index.core.agent.workflow import AgentWorkflow
from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI

# Initialize the Hugging Face model
llm = HuggingFaceInferenceAPI(model_name="Qwen/Qwen2.5-Coder-32B-Instruct")
# Create Alfred with all the tools
alfred = AgentWorkflow.from_tools_or_functions(
    [search_tool, weather_info_tool, hub_stats_tool],
    llm=llm
)

# Example query Alfred might receive during the gala
response = await alfred.run("Who is Meta AI and what's their most popular model?")

print("ðŸŽ© Alfred's Response:")
print(response)
```

Expected output: 

```python
ðŸŽ© Alfred's Response:
Meta AI, formerly known as Facebook AI, is a research lab focused on advancing and applying artificial intelligence to solve real-world problems. Their most popular model on the Hugging Face Hub is `facebook/esmfold_v1`, which has been downloaded 12,544,550 times.
```

</hfoption>
<hfoption id="langgraph">

```python
from typing import TypedDict, Annotated, Optional
from langchain.llms import HuggingFaceHub
from langgraph.graph.message import add_messages
from langgraph.graph import StateGraph
from langchain_core.messages import AnyMessage

# Define the state
class AgentState(TypedDict):
    response: Optional[str]
    messages: Annotated[list[AnyMessage], add_messages]

# Initialize the LLM
llm = HuggingFaceHub(
    repo_id="Qwen/Qwen2.5-Coder-32B-Instruct", 
    huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN
)

# Define the function to choose a tool
def tool_agent(data):
    query = data["messages"][0].content.lower()

    if "weather" in query:
        response = weather_info_tool.invoke(query)  # Example location
    elif "hugging face" in query:
        response = hub_stats_tool.invoke(query)
    else:
        response = search_tool.invoke(query)  # Default to web search

    return {"response": response}

# Set up LangGraph
builder = StateGraph(AgentState)

# Add the agent node
builder.add_node("agent", tool_agent)
builder.set_entry_point("agent")

# Compile the graph
graph = builder.compile()

# Example query Alfred might receive during the gala
response = graph.invoke({"messages": "Who is Meta AI and what's their most popular model?"})
print("ðŸŽ© Alfred's Response:")
print(response["response"])
```

Expected output:

```python
ðŸŽ© Alfred's Response:
According to the company, Meta AI is set to become the world's most widely used AI assistant by the end of the year. Over 400 million people interact with Meta AI monthly, with 185 million using ... Designed to make AI accessible to everyone, the AI model is built on Meta's open-source Llama 3.2 and works across the company's ecosystem allowing users to create, interact, and benefit from AI ... We tried seven of the most popular AI chatbots (ChatGPT, Claude, Grok, Perplexity and Arc Search, Meta AI, Microsoft Copilot) to find out. ... It uses a mix of models to produce its responses ... Yet, the difference is that Meta AI, the chatbot, integrated across Meta's social media platforms, has surpassed 500 million users since its wide release in April 2024. The rise of Meta AI. Meta AI's ascent to 500 million users in just seven months is a testament to its accessibility and utility. Impact of Meta AI on the Metaverse and VR/AR; History of Meta AI. Meta AI started as Facebook AI, focusing on AI research and innovation. In 2021, when Facebook rebranded to Meta, the scope of AI expanded further. Meta AI has made significant advancements in NLP (Natural Language Processing), computer vision, and deep learning. Use Cases of Meta AI
```
</hfoption>
</hfoptions>

## Conclusion

By integrating these tools, Alfred is now equipped to handle a variety of tasks, from web searches to weather updates and model statistics. This ensures he remains the most informed and engaging host at the gala.

<Exercise>
Try implementing a tool that can be used to get the latest news about a specific topic.

When you're done, implement your custom tools in the `tools.py` file.
</Exercise>