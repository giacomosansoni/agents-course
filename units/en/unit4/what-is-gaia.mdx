# What is GAIA?

[GAIA](https://huggingface.co/papers/2311.12983) is a **benchmark designed to evaluate AI assistants on real-world tasks** that require a blend of essential capabilitiesâ€”such as reasoning, multi-modal understanding, web browsing, and effective tool use.

It was introduced in the paper _"GAIA: A Benchmark for General AI Assistants"_.

The benchmark includes **466 questions** that are conceptually simple for humans but remain **highly challenging for current AI models**. To give you a sense of difficulty: humans achieve a **92% success rate**, while GPT-4 with plugins scores just **15%**.

## Difficulty Levels

GAIA tasks are organized into **three levels of increasing complexity**, each testing specific skills:

- **Level 1**: Requires around 5 steps and minimal tool usage.
- **Level 2**: Involves more complex reasoning and coordination between multiple tools.
- **Level 3**: Demands long-term planning and advanced integration of various tools.

## Example of a Hard GAIA Question

> Which of the fruits shown in the 2008 painting "Embroidery from Uzbekistan" were served as part of the October 1949 breakfast menu for the ocean liner that was later used as a floating prop for the film "The Last Voyage"? Give the items as a comma-separated list, ordering them in clockwise order based on their arrangement in the painting starting from the 12 o'clock position. Use the plural form of each fruit.

As you can see, this question challenges AI systems in several ways:

- Requires a **structured response format**
- Involves **multimodal reasoning** (e.g., analyzing images)
- Demands **multi-hop retrieval** of interdependent facts:
  - Identifying the fruits in the painting
  - Discovering which ocean liner was used in *The Last Voyage*
  - Looking up the breakfast menu from October 1949 for that ship
- Needs **correct sequencing** and high-level planning to solve in the right order

This kind of task highlights where standalone LLMs often fall short, making GAIA an ideal benchmark for **agent-based systems** that can reason, retrieve, and execute over multiple steps and modalities.

## Live Evaluation

To encourage continuous benchmarking, **GAIA provides a public leaderboard hosted on Hugging Face**, where you can test your models against **300 testing questions**.

ðŸ‘‰ Check out the leaderboard [here](https://huggingface.co/spaces/gaia-benchmark/leaderboard)

<iframe
	src="https://gaia-benchmark-leaderboard.hf.space"
	frameborder="0"
	width="850"
	height="450"
></iframe>

Want to dive deeper into GAIA?

- ðŸ“„ [Read the full paper](https://huggingface.co/papers/2311.12983)
- ðŸ“„ [Deep Research release post by OpenAI](https://openai.com/index/introducing-deep-research/)
- ðŸ“„ [Open-source DeepResearch â€“ Freeing our search agents](https://huggingface.co/blog/open-deep-research)