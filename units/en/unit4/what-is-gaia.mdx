## What is Gaia?

[GAIA (General AI Assistants)](https://huggingface.co/papers/2311.12983) is a **benchmark designed to evaluate AI systems on real-world tasks** that necessitate a combination of fundamental abilities, including reasoning, multi-modal understanding, web browsing, and proficient tool use. 

The benchmark comprises 466 questions that are conceptually straightforward for humans but present **significant challenges for current AI models**. For instance, human respondents achieve a 92% success rate on these tasks, whereas GPT-4 equipped with plugins attains only 15%. 

The GAIA benchmark is structured into **three difficulty levels**, each designed to test specific competencies of AI assistants:

- *Level 1* questions typically require around five steps and minimal tool use.
- *Level 2* involves more complex reasoning and multiple tools
- *Level 3* demands extensive multi-step planning and sophisticated tool integration.

To facilitate ongoing evaluation and improvement, **GAIA provides a public leaderboard hosted on Hugging Face**, where 300 of the questions are available for testing AI models. You can find the leaderboard[here](https://huggingface.co/spaces/gaia-benchmark/leaderboard)

If you want to learn more about GAIA, you can read the paper ðŸ‘‰[here](https://huggingface.co/papers/2311.12983)