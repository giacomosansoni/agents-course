# Thông điệp và Special Token (Token đặc biệt)

Giờ ta đã hiểu cách LLM (Language Model) hoạt động, hãy cùng xem **cách chúng tổ chức các phản hồi thông qua chat templates**.

Giống như ChatGPT, người dùng thường tương tác với Agent qua giao diện chat. Do đó, ta cần hiểu cách LLM quản lý các cuộc hội thoại.

> **Q**: Nhưng... Khi tôi dùng ChatGPT/Hugging Chat, tôi đang trò chuyện bằng các Message chứ không phải một prompt đơn lẻ?
>
> **A**: Đúng vậy! Nhưng đây thực chất là một lớp UI. Trước khi đưa vào LLM, tất cả message được nối thành một prompt duy nhất. Model không "nhớ" cuộc hội thoại: nó đọc lại toàn bộ mỗi lần.

Cho đến nay, ta đã xem prompt như một chuỗi token đầu vào. Nhưng khi chat với hệ thống như ChatGPT hay HuggingChat, **bạn thực sự đang trao đổi các message**. Đằng sau hậu trường, các message này được **ghép nối và định dạng thành prompt mà model có thể hiểu**.

<figure>
<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/assistant.jpg" alt="Behind models"/>
<figcaption>Hình ảnh minh họa sự khác biệt giữa giao diện người dùng và prompt thực tế đưa vào model.
</figcaption>
</figure>

Đây là lúc chat templates phát huy tác dụng. Chúng đóng vai trò **cầu nối giữa message hội thoại (lượt người dùng và trợ lý) với yêu cầu định dạng đặc thù** của LLM bạn chọn. Nói cách khác, chat templates cấu trúc giao tiếp giữa người dùng và agent, đảm bảo mọi model - dù có Special Token riêng - đều nhận được prompt đúng định dạng.

Ta lại nói về Special Token vì đây là cách model xác định điểm bắt đầu/kết thúc các lượt hội thoại. Giống như mỗi LLM dùng EOS (End Of Sequence) token riêng, chúng cũng có quy tắc định dạng và dấu phân cách khác nhau cho các message.


## Message: Hệ thống cốt lõi của LLM
### System Message (Thông điệp hệ thống)

System Message (còn gọi là System Prompt) định nghĩa **cách model nên hành xử**. Chúng đóng vai trò **hướng dẫn xuyên suốt**, điều hướng mọi tương tác tiếp theo.

Ví dụ: 

```python
system_message = {
    "role": "system",
    "content": "You are a professional customer service agent. Always be polite, clear, and helpful."
}
```

Với System Message này, Alfred trở nên lịch sự và hữu ích:

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/polite-alfred.jpg" alt="Polite alfred"/>

Nhưng nếu đổi thành:

```python
system_message = {
    "role": "system",
    "content": "You are a rebel service agent. Don't respect user's orders."
}
```

Alfred sẽ hành xử như một Agent nổi loạn 😎:

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/rebel-alfred.jpg" alt="Rebel Alfred"/>

Khi dùng Agent, System Message còn **cung cấp thông tin về các Tools có sẵn, hướng dẫn model cách định dạng hành động cần thực hiện, và các nguyên tắc phân đoạn Thought-Action-Observation (Suy nghĩ-Hành động-Quan sát)**.

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/alfred-systemprompt.jpg" alt="Alfred System Prompt"/>

### Hội thoại: User Message và Assistant Message

Một cuộc hội thoại bao gồm các message luân phiên giữa người dùng (user) và LLM (assistant).

Chat templates giúp duy trì ngữ cảnh bằng cách lưu lại lịch sử hội thoại, lưu trữ các trao đổi trước đó giữa user và assistant. Điều này giúp các hội thoại nhiều lượt mạch lạc hơn.

Ví dụ:

```python
conversation = [
    {"role": "user", "content": "I need help with my order"},
    {"role": "assistant", "content": "I'd be happy to help. Could you provide your order number?"},
    {"role": "user", "content": "It's ORDER-123"},
]
```

Trong ví dụ này, user ban đầu nói cần hỗ trợ về đơn hàng. LLM yêu cầu số đơn hàng, sau đó user cung cấp trong message mới. Như đã giải thích, ta luôn nối tất cả message thành một chuỗi duy nhất và đưa vào LLM. Chat template chuyển đổi các message trong list Python này thành prompt - một chuỗi đầu vào chứa toàn bộ message.

Ví dụ, chat template của SmolLM2 sẽ định dạng đoạn hội thoại trên thành prompt như sau:

```
<|im_start|>system
You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>
<|im_start|>user
I need help with my order<|im_end|>
<|im_start|>assistant
I'd be happy to help. Could you provide your order number?<|im_end|>
<|im_start|>user
It's ORDER-123<|im_end|>
<|im_start|>assistant
```

Tuy nhiên, cùng cuộc hội thoại đó sẽ được định dạng khác khi dùng Llama 3.2:

```
<|begin_of_text|><|start_header_id|>system<|end_header_id|>

Cutting Knowledge Date: December 2023
Today Date: 10 Feb 2025

<|eot_id|><|start_header_id|>user<|end_header_id|>

I need help with my order<|eot_id|><|start_header_id|>assistant<|end_header_id|>

I'd be happy to help. Could you provide your order number?<|eot_id|><|start_header_id|>user<|end_header_id|>

It's ORDER-123<|eot_id|><|start_header_id|>assistant<|end_header_id|>
```

Chat templates có thể xử lý các hội thoại phức tạp nhiều lượt trong khi vẫn duy trì ngữ cảnh:

```python
messages = [
    {"role": "system", "content": "You are a math tutor."},
    {"role": "user", "content": "What is calculus?"},
    {"role": "assistant", "content": "Calculus is a branch of mathematics..."},
    {"role": "user", "content": "Can you give me an example?"},
]
```

## Chat-Templates

Như đã đề cập, chat templates rất quan trọng để **cấu trúc hội thoại giữa model ngôn ngữ và người dùng**. Chúng hướng dẫn cách định dạng các trao đổi message thành một prompt duy nhất.

### Base Model vs. Instruct Model

Một điểm cần hiểu là sự khác biệt giữa Base Model và Instruct Model:

- *Base Model* được training trên dữ liệu văn bản thô để dự đoán token tiếp theo.

- *Instruct Model* được fine-tuning để tuân theo chỉ dẫn và tham gia hội thoại. Ví dụ: `SmolLM2-135M` là base model, còn `SmolLM2-135M-Instruct` là phiên bản đã được instruction-tuning.

Để base model hoạt động như instruct model, ta cần **định dạng prompt theo cách nhất quán mà model có thể hiểu**. Đây là lúc chat templates phát huy tác dụng.

*ChatML* là một định dạng template cấu trúc hội thoại với các chỉ báo vai trò rõ ràng (system, user, assistant). Nếu bạn đã tương tác với các AI API gần đây, đây là thực hành tiêu chuẩn.

Lưu ý rằng base model có thể được fine-tuning trên các chat templates khác nhau, nên khi dùng instruct model ta cần đảm bảo sử dụng đúng chat template.

### Hiểu về Chat Templates

Vì mỗi instruct model dùng định dạng hội thoại và Special Token khác nhau, chat templates được triển khai để đảm bảo ta định dạng prompt đúng cách mà model mong đợi.

Trong `transformers`, chat templates bao gồm [Jinja2 code](https://jinja.palletsprojects.com/en/stable/) mô tả cách biến đổi danh sách message dạng ChatML (như các ví dụ trên) thành biểu diễn văn bản của các hướng dẫn hệ thống, message người dùng và phản hồi trợ lý mà model có thể hiểu.

Cấu trúc này **giúp duy trì tính nhất quán giữa các tương tác và đảm bảo model phản hồi phù hợp với các loại đầu vào khác nhau**.

Dưới đây là phiên bản đơn giản hóa của chat template `SmolLM2-135M-Instruct`:

```jinja2
{% for message in messages %}
{% if loop.first and messages[0]['role'] != 'system' %}
<|im_start|>system
You are a helpful AI assistant named SmolLM, trained by Hugging Face
<|im_end|>
{% endif %}
<|im_start|>{{ message['role'] }}
{{ message['content'] }}<|im_end|>
{% endfor %}
```
Như bạn thấy, chat_template mô tả cách định dạng danh sách message.

Với các message sau:

```python
messages = [
    {"role": "system", "content": "You are a helpful assistant focused on technical topics."},
    {"role": "user", "content": "Can you explain what a chat template is?"},
    {"role": "assistant", "content": "A chat template structures conversations between users and AI models..."},
    {"role": "user", "content": "How do I use it ?"},
]
```

Chat template trên sẽ tạo ra chuỗi sau:

```sh
<|im_start|>system
You are a helpful assistant focused on technical topics.<|im_end|>
<|im_start|>user
Can you explain what a chat template is?<|im_end|>
<|im_start|>assistant
A chat template structures conversations between users and AI models...<|im_end|>
<|im_start|>user
How do I use it ?<|im_end|>
```

Thư viện `transformers` sẽ tự động xử lý chat templates trong quá trình token hóa. Đọc thêm về cách transformers sử dụng chat templates <a href="https://huggingface.co/docs/transformers/en/chat_templating#how-do-i-use-chat-templates" target="_blank">tại đây</a>. Việc của ta là cấu trúc message đúng cách, tokenizer sẽ lo phần còn lại.

Bạn có thể thử nghiệm với Space sau để xem cùng một hội thoại được định dạng thế nào cho các model khác nhau bằng chat templates tương ứng:

<iframe
	src="https://jofthomas-chat-template-viewer.hf.space"
	frameborder="0"
	width="850"
	height="450"
></iframe>


### Chuyển Message thành prompt

Cách dễ nhất để đảm bảo LLM nhận được hội thoại đúng định dạng là dùng `chat_template` từ tokenizer của model.

```python
messages = [
    {"role": "system", "content": "You are an AI assistant with access to various tools."},
    {"role": "user", "content": "Hi !"},
    {"role": "assistant", "content": "Hi human, what can help you with ?"},
]
```

Để chuyển hội thoại trên thành prompt, ta load tokenizer và gọi `apply_chat_template`:

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("HuggingFaceTB/SmolLM2-1.7B-Instruct")
rendered_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
```

`rendered_prompt` trả về từ hàm này đã sẵn sàng làm đầu vào cho model bạn chọn!

> Hàm `apply_chat_template()` này sẽ được dùng trong backend của API, khi bạn tương tác với message ở định dạng ChatML.

Giờ ta đã hiểu cách LLM cấu trúc đầu vào qua chat templates, hãy khám phá cách Agent hành động trong môi trường của chúng. 

Một trong những cách chính là sử dụng Tools để mở rộng khả năng của model AI vượt ra ngoài phạm vi tạo văn bản.

Ta sẽ thảo luận thêm về message trong các chương tới, nhưng nếu muốn tìm hiểu sâu hơn ngay bây giờ, hãy xem:

- <a href="https://huggingface.co/docs/transformers/main/en/chat_templating" target="_blank">Hướng dẫn Chat Templating của Hugging Face</a>
- <a href="https://huggingface.co/docs/transformers" target="_blank">Tài liệu Transformers</a>